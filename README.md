# ğŸš Yamama Rescue Drone â€“ AI-Powered Emergency Detection System

> A fully integrated **AI and automation project** combining **n8n**, **OpenAI API**, and **Gmail**, along with **local AI modules** for face detection and real-time translation, to support smart rescue operations using drones.

---

## ğŸ§­ Table of Contents
1. ğŸ§© Project Concept  
2. ğŸ¯ Objectives  
3. âš™ï¸ Components & Technologies  
4. ğŸ§  System Overview  
5. ğŸ”„ n8n Workflow  
6. ğŸ’¾ Dashboard (Figma Design)  
7. ğŸ™ï¸ Local AI Modules  
8. ğŸ§  AI Analysis Logic  
9. ğŸ–¼ï¸ Hardware & Visuals  
10. âš™ï¸ Execution Steps  
11. ğŸ‘¨â€ğŸ’» Team Credits  

---

## ğŸ§© Project Concept

**Yamama Rescue Drone** is an intelligent system designed to enhance **search and rescue missions** using artificial intelligence.  
It captures aerial images through a Raspberry Pi camera, analyzes them using **OpenAI API** through **n8n**,  
and also includes **local modules** that perform:
- Face detection and blurring for privacy  
- Real-time Arabic-to-English voice translation  

---

## ğŸ¯ Objectives

- Detect and classify emergency events (ğŸ”¥ Fire, ğŸš‘ Medical distress, ğŸ©¸ Bleeding).  
- Automate the process from **image capture â†’ AI analysis â†’ alert**.  
- Enable **local AI** for offline functionality (face detection and translation).  
- Support field operations with immediate feedback.  

---

## âš™ï¸ Components & Technologies

| Category | Tool / Device | Function |
|:--|:--|:--|
| ğŸ”„ Automation | **n8n** | Automates AI workflows and data handling. |
| ğŸ¤– AI Engine | **OpenAI API** | Performs image analysis and report generation. |
| ğŸ“§ Communication | **Gmail API** | Sends automatic reports and alerts. |
| ğŸ’» Processor | **Raspberry Pi 4** | Handles camera input and local AI scripts. |
| ğŸ“· Imaging | **IMX477 HQ Camera** | Captures high-quality aerial images. |
| ğŸ§  Local Vision | **OpenCV + Caffe** | Detects and blurs faces for privacy. |
| ğŸ™ï¸ Audio AI | **Whisper + gTTS** | Performs voice translation and text-to-speech. |
| ğŸš Drone Platform | **F450 Frame** | Physical drone body for flight and hardware mounting. |

---

## ğŸ§  System Overview

1ï¸âƒ£ **Image Capture:**  
Drone camera captures frames during operation.  

2ï¸âƒ£ **Cloud Analysis:**  
Images are sent to **n8n** â†’ **OpenAI API** â†’ analyzed for fire or distress.  

3ï¸âƒ£ **Local AI Processing:**  
Parallel modules on Raspberry Pi perform **face detection** and **voice translation**.  

4ï¸âƒ£ **Report Generation & Alert:**  
Results are formatted, summarized, and automatically emailed to the rescue team.

---

## ğŸ”„ n8n Workflow

ğŸ§© **Workflow Steps:**
1. **Execute Workflow** â€“ Manual or automatic trigger.  
2. **Edit Fields** â€“ Upload or receive an image.  
3. **Analyze Image (OpenAI)** â€“ Uses AI vision to describe image content.  
4. **Code Node** â€“ Formats structured text output.  
5. **Build Report** â€“ Creates a readable emergency summary.  
6. **IF Conditions** â€“ Filters (ğŸ”¥ Fire / ğŸš‘ Injury / ğŸ©¸ Bleeding).  
7. **Merge + Gmail Nodes** â€“ Combines data and sends automatic email alerts.  

ğŸ“¸ **n8n Workflow Preview:**  
![n8n Workflow](docs/images/n8n_workflow.png)

---

## ğŸ’¾ Dashboard (Figma Design)

A custom **Figma-based Dashboard** visualizes the systemâ€™s performance:  
- Displays live images from the drone.  
- Shows analysis progress and system status.  
- Highlights emergency detections with icons.  
- Lists recent alerts and email logs.

ğŸ“¸ **Dashboard Preview:**  
![Dashboard Preview](docs/images/dashboard_figma.png)

---

## ğŸ™ï¸ Local AI Modules

The project includes **standalone Python scripts** for real-time operations that run directly on the Raspberry Pi or any local PC.

### ğŸ§  1. Face Blur Detection
Detects faces in an image and automatically blurs them for privacy.

**Run Command:**
```bash
python face_blur.py
```

**Live Mode (Camera Stream):**
```bash
python live_face_blur.py
```

ğŸ“ Notes:
- Place your input image in the folder `example/5.jpeg`  
- Output will be saved as `example/output.jpg`  

---

### ğŸ§ 2. Speech Translator
Records Arabic speech, translates it into English using **Whisper + OpenAI + gTTS**, and plays back the result.

**Run Command:**
```bash
python speech_translate.py
```

**How it works:**
1. Press Enter to start.  
2. Speak Arabic for 5 seconds.  
3. Wait for transcription + translation.  
4. The system speaks the English translation out loud.  
5. Type `exit` to stop.  

---

## ğŸ§  AI Analysis Logic

The **OpenAI Vision API** inside **n8n** performs:
1. Textual description of the scene.  
2. Classification of potential hazards.  
3. Structured report generation.  
4. Automatic email notification.  

Local AI modules handle **face privacy** and **real-time speech translation**,  
while cloud AI handles **scene understanding** and **critical event detection**.

---

## ğŸ–¼ï¸ Hardware & Visuals

ğŸ“· **Camera IMX477 HQ**  
![Camera Module](docs/images/camera_imx477.jpg)

ğŸ§  **Raspberry Pi 4**  
![Raspberry Pi 4](docs/images/raspberry_pi4.jpg)

ğŸš **F450 Drone Frame**  
![Drone F450](docs/images/drone_f450.jpg)

âš™ï¸ **System in Action**  
![System Live](docs/images/system_live.jpg)

---

## âš™ï¸ Execution Steps

1ï¸âƒ£ Connect the **Raspberry Pi** and camera module.  
2ï¸âƒ£ Launch the **local AI modules** for face/speech analysis.  
3ï¸âƒ£ Start the **n8n workflow** for cloud AI analysis.  
4ï¸âƒ£ Capture or upload images to trigger detection.  
5ï¸âƒ£ Wait for both local and cloud systems to finish analysis.  
6ï¸âƒ£ Check your **email inbox** for the automated report.  

---

## ğŸ‘¨â€ğŸ’» Team Credits

**Project Name:** Yamama Rescue Drone  
**Team Name:** Team Yamama  

| Role | Name |
|:--|:--|
| ğŸ‘¨â€ğŸ« Project Advisor / Mentor | **Mr. Badr** |
| ğŸ‘¥ Team Members | *(Add team names here, e.g., Ali, Nasser, Salem, etc.)* |
| ğŸ§  Technical Supervision | **Abdulrahman Alnashri** |

---
